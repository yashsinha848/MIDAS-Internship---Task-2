{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Midas-2-2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcVWFExuahC6",
        "outputId": "b5a7a6f2-81ed-4e39-8ac1-fad5783c497d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv-T1HU9ay5P"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cblX_AdTa27L"
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/mnist-idx/mnist_train-1.csv')\n",
        "test = pd.read_csv('/content/drive/My Drive/mnist-idx/mnist_test-1.csv')\n",
        "#print(train.head())\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(train.iloc[:,1:],\n",
        "    train.iloc[:,0],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=train.iloc[:,0])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qazH8zibLK3"
      },
      "source": [
        "# We'll create a \"split\" of training, for cross-validation\n",
        "train_images_split = []\n",
        "train_labels_split = []\n",
        "validation_images = []\n",
        "validation_labels = []\n",
        "\n",
        "# We'll create a full training set\n",
        "train_images_full = []\n",
        "train_labels_full = []\n",
        "\n",
        "# And the test set\n",
        "test_images = []\n",
        "\n",
        "for index, row in x_train.iterrows():\n",
        "    train_images_split.append(row.values[0:].reshape((28, 28)))\n",
        "    train_labels_split.append(row.iloc[0])\n",
        "    train_images_full.append(row.values[0:].reshape((28, 28)))\n",
        "    train_labels_full.append(row.iloc[0])\n",
        "\n",
        "for index, row in x_validation.iterrows():\n",
        "    validation_images.append(row.values[0:].reshape((28, 28)))\n",
        "    validation_labels.append(row.iloc[0])\n",
        "    train_images_full.append(row.values[0:].reshape((28, 28)))\n",
        "    train_labels_full.append(row.iloc[0])\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    test_images.append(row.iloc[1:].values.reshape((28, 28)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBBU1TCXbR6c",
        "outputId": "ce116d08-8494-453d-a4b3-cf8d72bb7cbf"
      },
      "source": [
        "# Convert numpy array, while normalizing the image data\n",
        "train_labels_split = np.array(train_labels_split)\n",
        "train_images_split = np.array(train_images_split) / 255.\n",
        "validation_labels = np.array(validation_labels)\n",
        "validation_images = np.array(validation_images) / 255.\n",
        "train_labels_full = np.array(train_labels_full)\n",
        "train_images_full = np.array(train_images_full) / 255.\n",
        "test_images = np.array(test_images) / 255.\n",
        "\n",
        "print(train_labels_full.shape)\n",
        "print(train_images_full.shape)\n",
        "print(train_labels_split.shape)\n",
        "print(train_images_split.shape)\n",
        "print(validation_labels.shape)\n",
        "print(validation_images.shape)\n",
        "print(test_images.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(59999,)\n",
            "(59999, 28, 28)\n",
            "(47999,)\n",
            "(47999, 28, 28)\n",
            "(12000,)\n",
            "(12000, 28, 28)\n",
            "(9999, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7oUPfeYbW3J"
      },
      "source": [
        "train_images = np.expand_dims(train_images_split, axis=3)\n",
        "validation_images = np.expand_dims(validation_images, axis=3)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Um6x0ZXPbhFQ",
        "outputId": "9440990d-92a2-4898-91aa-b64d897f94f0"
      },
      "source": [
        "pre_trained_model =load_model('/content/drive/My Drive/small_train/checkpoint_accuracy281.hdf5')\n",
        "pre_trained_model.trainable = False\n",
        "#print(pre_trained_model.summary())\n",
        "last_layer = pre_trained_model.get_layer('activation_11')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import Model\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# # Add a final sigmoid layer for classification\n",
        "x = layers.Dense(10, activation='softmax')(x)\n",
        "#\n",
        "model = Model(pre_trained_model.input, x)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "last layer output shape:  (None, 128)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvW4SepGbsOb"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Generators\n",
        "train_gen = ImageDataGenerator().flow(train_images, train_labels_split, batch_size=32)\n",
        "validation_gen = ImageDataGenerator().flow(validation_images, validation_labels, batch_size=32)\n",
        "checkpointer = keras.callbacks.ModelCheckpoint(filepath = r'/content/drive/My Drive/small_train/checkpoint-mnist-transfer-learning.hdf5',\n",
        "                                               monitor='accuracy',\n",
        "                                               mode='min',\n",
        "                                               save_best_only=False,\n",
        "                                               verbose=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnw1c9vFbw6H",
        "outputId": "88523ef2-a9cb-4edd-958e-33af801d45ee"
      },
      "source": [
        "\n",
        "# Model fit\n",
        "history = model.fit(train_gen,\n",
        "                    validation_data = validation_gen,\n",
        "                    steps_per_epoch = len(train_images) / 32,\n",
        "                    validation_steps = len(validation_images) / 32,\n",
        "                    epochs=5,\n",
        "                    callbacks = [checkpointer])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "1499/1499 [==============================] - 23s 16ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00001: saving model to /content/drive/My Drive/small_train/checkpoint-mnist-transfer-learning.hdf5\n",
            "Epoch 2/5\n",
            "1499/1499 [==============================] - 23s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/small_train/checkpoint-mnist-transfer-learning.hdf5\n",
            "Epoch 3/5\n",
            "1499/1499 [==============================] - 23s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00003: saving model to /content/drive/My Drive/small_train/checkpoint-mnist-transfer-learning.hdf5\n",
            "Epoch 4/5\n",
            "1499/1499 [==============================] - 23s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/small_train/checkpoint-mnist-transfer-learning.hdf5\n",
            "Epoch 5/5\n",
            "1499/1499 [==============================] - 23s 15ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
            "\n",
            "Epoch 00005: saving model to /content/drive/My Drive/small_train/checkpoint-mnist-transfer-learning.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}